# COSN-Hackathon2025

## Experiment paradigm

![exp1](https://github.com/nmningmei/COSN-Hackathon2025/blob/main/figures/exp1-paradigm.png)

每个试次开始时，屏幕上呈现一个持续0.5秒的“+”号注视点。在0.5s至1.5s的随机空白间隔之后，呈现目标图片(面孔或房子)，呈现时间仅为1帧(16.67毫秒)，随后立即被5帧(82.35毫秒)的高斯噪声图片覆盖。之后再次呈现0.5至1.5秒的随机空白间隔。随后，被试需依次报告目标图片类别(面孔或房子)及当前主观意识状态(无意识、部分意识或有意识)。当屏幕显示出“V_nV”或“nV_V”时(两种提示随机呈现)，被试根据“V”和“nV”的左右位置，通过右手食指或中指按键来报告图片类别，其中“V”代表面孔，“nV”代表房子；被试必须在1.5秒内完成按键反应。之后，屏幕呈现“？”，被试需要使用食指、中指和无名指分别对应报告无意识、部分意识和有意识状态，同样需要在1.5秒内完成按键反应。被试完成按键后不会进入下一屏而是持续1.5秒后结束，被试可以在1.5秒内更改按键。每个试次结束时会呈现1至2秒的随机空白屏。根据被试报告的主观意识状态，下一试次的目标图片对比度将相应增强或减弱。被试需完成8组连续实验区块，每组包含50张面孔图片与50张房子图片，总计800个试次。通过对目标图片强度进行阶梯式增强/减弱设计，确保总体上被试报告的无意识试次占比约为50%。实验过程中记录下被试对目标图片类别的按键反应、反应时以及对主观意识状态的按键反应及其反应时。在每个实验区块间，被试有权选择休息或是继续进行实验。实验图片呈现设备为电脑屏幕，距离被试双眼中心90厘米，图片居于屏幕中央。

## MEG searchligh decoding

![searchlight](https://github.com/nmningmei/COSN-Hackathon2025/blob/main/figures/searchligh_decoding.png)

[Sato et al., PLOS one, 2018](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0198806)